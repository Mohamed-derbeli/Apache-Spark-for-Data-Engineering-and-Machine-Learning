{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504d51d2-5ea8-4fad-bd6f-4d8b4ef86f72",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2021-03-17T17:21:55.111968",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.106622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spark RandomForest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35583771-c953-476c-9a62-0e44bc7d5b56",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2021-03-17T17:21:55.111968",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.106622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this task, we will build an end to end pipeline that reads in data in parquet format, converts it to CSV and loads it into a dataframe, trains a model and perform hyperparameter tuning. This notebook does the following:\n",
    "\n",
    "*   Read in the `parquet` file.\n",
    "\n",
    "*   Convert the `parquet` file to `CSV` format.\n",
    "\n",
    "*   Load the CSV file into a dataframe\n",
    "\n",
    "*   Create a 80-20 training and test split with `seed=1`.\n",
    "\n",
    "*   Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations.\n",
    "\n",
    "    Hyper parameters:\n",
    "\n",
    "    ```\n",
    "      - number of trees : {10, 20}\n",
    "      - maximum depth : {5, 7} \n",
    "      - use random seed = 1 wherever needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e5577-edc2-475a-8c1d-788082a37858",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2021-03-17T17:21:55.111968",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.106622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Converts a parquet file to CSV file with header using ApacheSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d16380-ad14-4a38-bc4b-5f5782681a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b167d77-ed0c-4046-a014-22a883be099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ef0869-8875-4fec-8772-dbe5ebb3c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27933eb4-1bda-4fea-9ad2-173653fd7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv') #data_csv csv path and file name\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet') # input file name (parquet)\n",
    "master = os.environ.get('master', \"local[*]\")  # URL to Spark master\n",
    "data_dir = os.environ.get('data_dir', '../../data/') # temporary directory for data\n",
    "model_target = os.environ.get('model_target', \"model.xml\")  # model output file name\n",
    "input_columns = os.environ.get('input_columns', '[\"x\", \"y\", \"z\"]')  # input columns to consider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20a2b7d-7700-48f0-8ca7-ced0effc8916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/10 00:17:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "# if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13ecc3f-3d53-442f-8555-327563140e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = 'trends.parquet'\n",
    "data_csv = 'trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd65875-240c-487a-b81a-fad723f47a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32ca2b4-7a2c-48a7-bf4f-55e934a1112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2a1d15-617a-4150-939f-21d5b9735768",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26235c67-4b1f-476d-bf2e-0576ae68c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override parameters received from a potential call using %run magic\n",
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a318f8b-5ce7-4142-aff0-301b37a17556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92e9542-d478-4346-8656-63ed1475c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df1.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63626536-6d85-4b66-9e42-4d84f62490fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+--------+\n",
      "|_c0|_c1|_c2|                 _c3|     _c4|\n",
      "+---+---+---+--------------------+--------+\n",
      "|  x|  y|  z|              source|   class|\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 31| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 38| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 34| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "+---+---+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e055393-732c-4dea-a6ba-2da29b6f3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.option('header', 'true').csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4580d363-51df-411b-805e-92aa58bed806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+--------+\n",
      "|  x|  y|  z|              source|   class|\n",
      "+---+---+---+--------------------+--------+\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 31| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 38| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 34| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "+---+---+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9aa775b-e900-451c-9db3-7fa22ddb1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df1.createOrReplaceTempView('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16473c06-b57d-4b57-8792-71ecd751de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df1 = df1.withColumn(\"x\", df1.x.cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"y\", df1.y.cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"z\", df1.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f65303b-f9a7-4f44-afa5-ee5910150785",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df1.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15046455-75f9-449a-acdf-4668c9c4489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72e9d405-688a-4f96-a0fe-d689603b3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 10 maxDepth = 5 : Accuracy = 0.4402756422486841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 10 maxDepth = 7 : Accuracy = 0.46222825781977755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 20 maxDepth = 5 : Accuracy = 0.4426437413721507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:============================>                            (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 20 maxDepth = 7 : Accuracy = 0.466414518355574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for nT in [10, 20]:\n",
    "    for maxD in [5,7]:\n",
    "        rf = RandomForestClassifier(numTrees=nT, maxDepth=maxD, seed=1)\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_test)\n",
    "        binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "        this_pred = binEval.evaluate(prediction)\n",
    "        print(\"numTrees =\",nT,\"maxDepth =\",maxD,\": Accuracy =\", this_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaaeab-62d3-4b6b-9dbb-08c2e4cbce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmmlBuilder = PMMLBuilder(sc, df_train, model)\n",
    "# pmmlBuilder.buildFile(data_dir + model_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
